<div align="center"><h1> 淡蓝小点技术系列：统计学习理论的本质精炼介绍 </h1></div>

本文件有待进一步完善

## 关于统计学习理论的本质精炼介绍
淡蓝小点技术系列：学习理论精炼介绍

让我们先来思考几个问题

问题一：
众所周知，概率统计的两大核心依托是：大数定律和中心极限定理但是这两大依托讨论的都是样本数量趋于无穷时试验中所呈现的规律，现实中，我们往往只能采集到数据集中极小的一部分，那么，基于概率统计建立的统计学习，其理论基础是否可靠了？

问题二：
准备一个训练数据集，基于这个训练集通过最小化损失函数就能学到一个可用模型，每个人都在这么做，但是否有谁论证过这样做真的是合理的了？加入正则化项就能避逸过拟合，有人能说清楚为什么了？基于训练集得到的模型，它在训练集上的误差和在泛化数据上的误差，二者有多大差距了？这个差距是否可控、可量化了？

问题三：
如果我说：线性模型是最简单的模型，二次模型表现力比线性模型要强，三次模型比二次模型强，四次模型比三次模型强，我想你肯定会同意。但是所谓”模型表现力“到底是什么了？在数学的世界里，它肯定不应该只是一个模糊的概念，它应该是一个具体的、定义清晰的数学量，但这个数学量是什么了？是模型中参数的数量？是模型最高阶的次数？是模型的可导次数？我们如何度量模型的”表现力“？

问题四：
什么是小样本集？多小的样本集是小样本集？50个？100个？还是1000个、10000个？为什么越复杂的函数越容易过拟合了？每本书都在这么教我们，但是有谁能证明这一点了？

学习理论（Learning theory）回答的是机器学习中最基本最普适的问题，它是其它所有机器学习理论的理论之母。

形而上者谓之道，形而下者谓之器。
线性判定模型、人工神经网络、PCA、LDA、HMM、随机梯度下降、BP算法、EM、变分法...这些都只是器，抑或神器，亦止于器。而学习理论正是机器学习问题的道！

佛曰：凡所有相，皆为虚妄；若见诸相非相，即见如来。
道曰：道可道，非常道；名可名，非常名

如果上述问题不能被回答，整个机器学习的根基就是可动摇的、不坚实的，这就尤如当年漂亮宏大的微积分大厦，其根基却被幽灵一样的无穷小量啃噬

最深奥的问题就是最朴素的问题，最朴素的问题就是最深奥的问题

如果你想有完整全面的机器学习理论基础，如果你想在机器学习、深度学习、人工智能领域上深耕十年、二十年，请关注淡蓝小点的《学习理论精炼介绍》专题。

## 项目视频介绍
https://www.bilibili.com/video/BV1wD421N7pG/?vd_source=633e638fa734046bec979a87294ad6bd

## 开始时间
预计两周之内，请入群或关注朋友圈（更新日期：2024/5/12）

## 谁是淡蓝小点
淡蓝小点是PRML Page-by-page项目的发起人，这是一个旨在帮助机器学习从业人员读懂PRML每一页的项目，它包括约80小时的视频和一本1000页的note，可通过下面链接找到相关视频。若想要note请加微信索取。

PRML Page-by-page：https://space.bilibili.com/353555504?spm_id_from=333.1007.0.0

微信号及二维码：bluedotdot_cn

<img src="wechat.jpg" alt="淡蓝小点微信二维码" width="150" height="150">
